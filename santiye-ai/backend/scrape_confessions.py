"""
KÄ±zlar Soruyor Scraper - Ä°tiraf Feed Data Populator
====================================================
Bu script kizlarsoruyor.com'dan iliÅŸki iÃ§eriklerini Ã§eker ve 
Supabase confessions tablosuna ekler.

KullanÄ±m:
1. pip install requests beautifulsoup4 python-dotenv supabase
2. .env dosyasÄ±nda SUPABASE_URL ve SUPABASE_KEY ayarlayÄ±n
3. python scrape_confessions.py
"""

import os
import time
import random
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from supabase import create_client, Client

# Load environment variables from multiple possible locations
from pathlib import Path

# Try to find .env file in various locations
env_paths = [
    Path(__file__).parent / ".env",
    Path(__file__).parent.parent / "frontend" / ".env.local",
    Path(__file__).parent.parent / ".env.local",
]

for env_path in env_paths:
    if env_path.exists():
        load_dotenv(env_path)
        print(f"ğŸ“ Loaded env from: {env_path}")
        break

# Supabase config
SUPABASE_URL = os.getenv("NEXT_PUBLIC_SUPABASE_URL") or os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("NEXT_PUBLIC_SUPABASE_ANON_KEY") or os.getenv("SUPABASE_KEY")

# Interactive fallback if env vars not found
if not SUPABASE_URL:
    print("âš ï¸ SUPABASE_URL bulunamadÄ±. LÃ¼tfen manuel girin:")
    SUPABASE_URL = input("Supabase URL: ").strip()

if not SUPABASE_KEY:
    print("âš ï¸ SUPABASE_KEY bulunamadÄ±. LÃ¼tfen manuel girin:")
    SUPABASE_KEY = input("Supabase Anon Key: ").strip()

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ Hata: Supabase bilgileri eksik!")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# Browser-like headers to avoid 403
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Accept-Language": "tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
}

# Categories to scrape
CATEGORIES = [
    ("https://www.kizlarsoruyor.com/iliski-sorunlari", "Ä°liÅŸki"),
    ("https://www.kizlarsoruyor.com/erkek-kiz-iliskileri", "FlÃ¶rt"),
    ("https://www.kizlarsoruyor.com/evlilik", "Evlilik"),
]

def fetch_page(url):
    """Fetch a page with browser-like headers"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"âš ï¸ Sayfa alÄ±namadÄ±: {url} - {e}")
        return None

def parse_questions(html, category):
    """Parse questions from the page HTML"""
    soup = BeautifulSoup(html, "html.parser")
    questions = []
    
    # Find question items - adjust selectors based on actual site structure
    question_items = soup.select(".question-item, .feed-item, article.question")
    
    if not question_items:
        # Try alternative selectors
        question_items = soup.select("[class*='question'], [class*='post'], .listItem")
    
    for item in question_items[:20]:  # Limit to 20 per category
        try:
            # Try to find title/hook
            title_el = item.select_one("h2, h3, .question-title, .title, a.question-link")
            content_el = item.select_one(".question-content, .excerpt, .description, p")
            
            title = title_el.get_text(strip=True) if title_el else None
            content = content_el.get_text(strip=True) if content_el else ""
            
            if title and len(title) > 10:
                # Combine title and content
                full_content = f"{title}\n\n{content}" if content else title
                
                questions.append({
                    "content": full_content[:2000],  # Limit length
                    "category": category,
                    "toxic_score": random.randint(40, 95),  # Random for demo
                    "like_count": random.randint(10, 500),
                })
        except Exception as e:
            print(f"âš ï¸ Parse hatasÄ±: {e}")
            continue
    
    return questions

def insert_to_supabase(questions):
    """Insert questions into Supabase confessions table"""
    if not questions:
        return 0
    
    try:
        result = supabase.table("confessions").insert(questions).execute()
        return len(result.data) if result.data else 0
    except Exception as e:
        print(f"âŒ Supabase insert hatasÄ±: {e}")
        return 0

def generate_sample_data():
    """Generate sample Turkish relationship confessions if scraping fails"""
    samples = [
        {
            "content": "Sevgilim beni aldatÄ±yor mu bilmiyorum ama sÃ¼rekli telefonunu saklÄ±yor\n\n3 yÄ±llÄ±k iliÅŸkimiz var. Son zamanlarda telefonunu sÃ¼rekli ekranÄ± aÅŸaÄŸÄ± bakacak ÅŸekilde koyuyor. Åifresini deÄŸiÅŸtirmiÅŸ, sormaya korkuyorum. Paranoyak mÄ±yÄ±m yoksa gerÃ§ekten bir ÅŸeyler mi dÃ¶nÃ¼yor?",
            "category": "Aldatma",
            "toxic_score": 85,
            "like_count": random.randint(100, 500),
        },
        {
            "content": "Eski sevgilimi unutamÄ±yorum yeni iliÅŸkiye baÅŸladÄ±m ama hala onu dÃ¼ÅŸÃ¼nÃ¼yorum\n\n6 ay Ã¶nce ayrÄ±ldÄ±k. Yeni biriyle tanÄ±ÅŸtÄ±m, Ã§ok iyi biri ama aklÄ±m hep eskide. Bu yeni insana haksÄ±zlÄ±k mÄ± ediyorum? NasÄ±l unuturum?",
            "category": "AyrÄ±lÄ±k",
            "toxic_score": 45,
            "like_count": random.randint(50, 300),
        },
        {
            "content": "Erkek arkadaÅŸÄ±m eski sevgilisiyle hala konuÅŸuyor, 'sadece arkadaÅŸÄ±z' diyor\n\nBunu kabul etmem mi gerekiyor? Her hafta kahve iÃ§meye gidiyorlar. RahatsÄ±z olduÄŸumu sÃ¶yledim ama 'abartÄ±yorsun' dedi.",
            "category": "Red Flags",
            "toxic_score": 78,
            "like_count": random.randint(200, 600),
        },
        {
            "content": "Sevgilim sosyal medyada baÅŸka kÄ±zlarÄ±n fotoÄŸraflarÄ±nÄ± beÄŸeniyor\n\nBikini fotoÄŸraflarÄ±, makyaj videolarÄ±... SorduÄŸumda 'ne var bunda' diyor. AÅŸÄ±rÄ± mÄ± tepki veriyorum?",
            "category": "GÃ¼ven",
            "toxic_score": 62,
            "like_count": random.randint(80, 400),
        },
        {
            "content": "4 yÄ±llÄ±k iliÅŸkiden sonra evlilik teklifi gelmedi, beklemeli miyim?\n\nO 'daha erken' diyor ama yaÅŸÄ±m ilerliyor. Ultimatom vermeli miyim yoksa beklemeli miyim? Kafam Ã§ok karÄ±ÅŸÄ±k.",
            "category": "Evlilik",
            "toxic_score": 35,
            "like_count": random.randint(150, 450),
        },
        {
            "content": "Ä°lk buluÅŸmada Ã¶pÃ¼ÅŸtÃ¼k, ÅŸimdi mesajlarÄ±ma cevap vermiyor\n\nHer ÅŸey Ã§ok gÃ¼zel gitti sanÄ±yordum. Neden ghostladÄ±? Ne yanlÄ±ÅŸ yaptÄ±m anlamÄ±yorum.",
            "category": "FlÃ¶rt",
            "toxic_score": 55,
            "like_count": random.randint(60, 250),
        },
        {
            "content": "Sevgilimin ailesini hiÃ§ sevmiyorum, evlensek bile ayrÄ± yaÅŸamak istiyorum\n\nAnlayÄ±ÅŸsÄ±z ve mÃ¼dahaleciler. Onu seviyorum ama ailesiyle aynÄ± ÅŸehirde bile yaÅŸamak istemiyorum. Bencil miyim?",
            "category": "Aile",
            "toxic_score": 48,
            "like_count": random.randint(90, 350),
        },
        {
            "content": "AldatÄ±ldÄ±m ama onu hala seviyorum, affetmeli miyim?\n\nBir kerelik hata olduÄŸunu sÃ¶ylÃ¼yor, Ã§ok piÅŸman. 5 yÄ±llÄ±k iliÅŸkiyi bu yÃ¼zden bitirebilir miyim? Kafam Ã§ok karÄ±ÅŸÄ±k.",
            "category": "Aldatma",
            "toxic_score": 92,
            "like_count": random.randint(300, 800),
        },
        {
            "content": "Uzak mesafe iliÅŸki yapÄ±yoruz, artÄ±k dayanamÄ±yorum\n\n2 yÄ±ldÄ±r farklÄ± ÅŸehirlerdeyiz. Ayda bir gÃ¶rÃ¼ÅŸÃ¼yoruz. Onu seviyorum ama bu bÃ¶yle devam edemez. Ne yapmalÄ±yÄ±m?",
            "category": "Uzak Mesafe",
            "toxic_score": 40,
            "like_count": random.randint(100, 400),
        },
        {
            "content": "Erkek arkadaÅŸÄ±m kÄ±skanÃ§lÄ±k krizleri geÃ§iriyor, normal mi?\n\nErkek arkadaÅŸlarÄ±mla konuÅŸmamÄ± istemiyor. KÄ±yafetlerime karÄ±ÅŸÄ±yor. 'Ã‡ok sevdiÄŸim iÃ§in' diyor ama bu saÄŸlÄ±klÄ± mÄ±?",
            "category": "ManipÃ¼lasyon",
            "toxic_score": 88,
            "like_count": random.randint(200, 600),
        },
    ]
    return samples

def main():
    print("ğŸš€ KÄ±zlar Soruyor Scraper BaÅŸlatÄ±lÄ±yor...")
    print(f"ğŸ“Š Supabase URL: {SUPABASE_URL[:30]}...")
    
    total_inserted = 0
    
    # Try to scrape each category
    for url, category in CATEGORIES:
        print(f"\nğŸ“¥ Kategori: {category} - {url}")
        
        html = fetch_page(url)
        if html:
            questions = parse_questions(html, category)
            if questions:
                inserted = insert_to_supabase(questions)
                total_inserted += inserted
                print(f"   âœ… {inserted} itiraf eklendi")
            else:
                print(f"   âš ï¸ Soru bulunamadÄ±, sayfa yapÄ±sÄ± deÄŸiÅŸmiÅŸ olabilir")
        
        # Rate limiting
        time.sleep(random.uniform(1, 3))
    
    # If scraping didn't work, insert sample data
    if total_inserted == 0:
        print("\nâš ï¸ Web scraping baÅŸarÄ±sÄ±z (site engelliyor olabilir)")
        print("ğŸ“ Ã–rnek veriler ekleniyor...")
        
        samples = generate_sample_data()
        inserted = insert_to_supabase(samples)
        total_inserted = inserted
        print(f"   âœ… {inserted} Ã¶rnek itiraf eklendi")
    
    print(f"\nğŸ‰ TamamlandÄ±! Toplam {total_inserted} itiraf eklendi.")
    print("   Feed'i yenileyerek kontrol edebilirsiniz: https://askanaliz.com/confessions")

if __name__ == "__main__":
    main()
